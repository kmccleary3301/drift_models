# Paper Table 8 exact template: "ablation default" column.
# Note: this config encodes paper-facing hyperparameters; running at full scale
# is extremely compute/memory intensive.

steps: 30000
log-every: 50
save-every: 1000

device: auto
seed: 1337
precision: bf16

# Table 8 (Generator Architecture): DiT-B/2 latent
num-classes: 1000
image-size: 32
channels: 4
patch-size: 2
hidden-dim: 768
depth: 12
num-heads: 12
mlp-ratio: 4.0
# FFN inner dimension override to match the paper-reported DiT-B/2-like parameter count (~133M, excluding SD-VAE).
ffn-inner-dim: 2184
register-tokens: 16
norm-type: rmsnorm
use-qk-norm: true
use-rope: true

temperature: 0.05
drift-temperatures: [0.02, 0.05, 0.2]

# Table 8 (Generator Optimizer)
learning-rate: 2e-4
adam-beta1: 0.9
adam-beta2: 0.95
weight-decay: 0.01
scheduler: warmup_cosine
warmup-steps: 5000
use-ema: true
ema-decay: 0.999

# Table 8 (CFG Configuration)
alpha-min: 1.0
alpha-max: 4.0
alpha-dist: powerlaw
alpha-power: 3.0
unconditional-per-group: 16

# Table 8 (Drifting Loss Computation)
# Ni=64, Nseg=64, Npos=64 => effective batch B=4096.
groups: 64
negatives-per-group: 64
positives-per-group: 64

# Feature encoder path used for drifting loss (latent-MAE, ablation default width/epochs differ from Table 5).
use-feature-loss: true
feature-encoder: mae
mae-encoder-path: outputs/imagenet/mae_variant_a_w256/mae_encoder.pt
mae-encoder-arch: paper_resnet34_unet
# Table 8 lists ResNet base width=256 for ablation default; keep this explicit for parity tracking.
feature-base-channels: 256
feature-stages: 4
feature-temperatures: [0.02, 0.05, 0.2]
feature-temperature-aggregation: sum_drifts_then_mse
feature-loss-term-reduction: sum
include-input-x2-mean: true
include-patch4-stats: true
feature-include-raw-drift-loss: true
feature-raw-drift-loss-weight: 1.0

# Queue + real batches
use-queue: true
queue-prime-samples: 50000
queue-push-batch: 64
queue-per-class-capacity: 128
queue-global-capacity: 1000
queue-store-device: cpu
queue-warmup-mode: class_balanced
queue-warmup-min-per-class: 1
queue-strict-without-replacement: true
queue-report-level: full

real-batch-source: tensor_shards
real-loader-batch-size: 4096
real-num-workers: 0
real-sanity-sample-batches: 0
real-tensor-shards-manifest-path: outputs/datasets/imagenet1k_train_sdvae_latents_shards/manifest.json

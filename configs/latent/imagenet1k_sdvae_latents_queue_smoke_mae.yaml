# Smoke-level ImageNet-1k latent run (SD-VAE 32x32x4 latents) with MAE feature loss.
# This is not paper-scale; it is meant to validate end-to-end wiring on real ImageNet latents.

steps: 60
log-every: 5
save-every: 0

device: auto
seed: 1337
precision: bf16

num-classes: 1000
image-size: 32
channels: 4

groups: 8
negatives-per-group: 4
positives-per-group: 4

alpha-min: 1.0
alpha-max: 4.0
alpha-dist: table8_l2_latent
alpha-power: 3.0
alpha-point: 1.0
alpha-point-prob: 0.5

patch-size: 2
hidden-dim: 1024
depth: 24
num-heads: 16
mlp-ratio: 4.0
register-tokens: 16
norm-type: rmsnorm
use-qk-norm: true
use-rope: true

temperature: 0.05
drift-temperatures: [0.02, 0.05, 0.2]
learning-rate: 1e-4
scheduler: none
warmup-steps: 0

use-feature-loss: true
feature-encoder: mae
# Default MAE encoder selection (DEC-0026): w64 variant.
mae-encoder-path: outputs/imagenet/mae_variant_a_w64/mae_encoder.pt
mae-encoder-arch: resnet_unet
feature-base-channels: 64
feature-stages: 4
feature-temperatures: [0.02, 0.05, 0.2]

use-queue: true
unconditional-per-group: 2
queue-prime-samples: 500
queue-per-class-capacity: 128
queue-global-capacity: 1000
queue-store-device: cpu
queue-warmup-mode: random
queue-report-level: full

real-batch-source: tensor_shards
real-loader-batch-size: 128
real-num-workers: 0
real-sanity-sample-batches: 2
real-tensor-shards-manifest-path: outputs/datasets/imagenet1k_train_sdvae_latents_shards/manifest.json
